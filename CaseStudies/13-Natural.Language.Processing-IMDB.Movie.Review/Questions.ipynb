{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SeqNLP_Project1_Questions.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xT7MKZuMRaCg"},"source":["# Sentiment Classification\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wq4RCyyPSYRp"},"source":["## Loading the dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NGCtiXUhSWss","colab":{}},"source":["from keras.datasets import imdb\n","\n","vocab_size = 10000 #vocab size\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCPC_WN-eCyw","colab_type":"code","colab":{}},"source":["from keras.preprocessing.sequence import pad_sequences\n","vocab_size = 10000 #vocab size\n","maxlen = 300  #number of word used from each review"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qMEsHYrWxdtk","colab_type":"text"},"source":["## Train test split"]},{"cell_type":"code","metadata":{"id":"h0g381XzeCyz","colab_type":"code","colab":{}},"source":["#load dataset as a list of ints\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n","#make all sequences of the same length\n","x_train = pad_sequences(x_train, maxlen=maxlen)\n","x_test =  pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jy6n-uM2eCy2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tZhMAgaNeCy5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dybtUgUReCy8","colab_type":"text"},"source":["## Build Keras Embedding Layer Model\n","We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n","\n","* The embedding layer can be used at the start of a larger deep learning model. \n","* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n","* Use the embedding layer to train our own word2vec models.\n","\n","The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."]},{"cell_type":"code","metadata":{"id":"A5OLM4eBeCy9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxNDNhrseCzA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3CSVVPPeCzD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Igq8Qm8GeCzG","colab_type":"text"},"source":["## Retrive the output of each layer in keras for a given single test sample from the trained model you built"]},{"cell_type":"code","metadata":{"id":"0AqOnLa2eCzH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dUDSg7VeCzM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tskt_1npeCzP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
