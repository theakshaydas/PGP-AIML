{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baby-sitting the Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        output = np.dot(self.X, self.W) + self.b\n",
    "        return output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "        self.gradB = np.sum(nextgrad, axis=0)\n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Rectified Linear Activation Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradInput = nextgrad.copy()\n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()        \n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset : Lets us load the training and the test data and check the size of the tensors. Lets us also display the first few images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "(train_features, train_targets), (test_features, test_targets) = mnist.load_data()\n",
    "\n",
    "train_features = train_features.reshape(60000, 784)\n",
    "print train_features.shape\n",
    "test_features = test_features.reshape(10000, 784)\n",
    "print test_features.shape\n",
    "\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "train_features = train_features / 255.0\n",
    "test_features = test_features / 255.0\n",
    "\n",
    "print train_targets.shape\n",
    "print test_targets.shape\n",
    "\n",
    "X_train = train_features\n",
    "y_train = train_targets\n",
    "\n",
    "X_val = test_features\n",
    "y_val = test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABSCAYAAABwglFkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d78bb9ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the above image: [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the first 10 images in the dataset and their labels\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label for each of the above image: %s' % (y_train[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we define the container NN class that enables the forward prop and backward propagation of the entire network. Note, how this class enables us to add layers of different types and also correctly pass gradients using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, lossfunc=CrossEntropy(), mode='train'):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.loss_func = lossfunc\n",
    "        self.grads = []\n",
    "        self.mode = mode\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out,y)  + ((Lambda / (2 * y.shape[0])) * np.sum([np.sum(w**2) for w in self.params[0][0]]))\n",
    "        nextgrad = self.loss_func.backward(out,y) + ((Lambda/y.shape[0]) * np.sum([np.sum(w) for w in self.params[0][0]]))\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return p\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the update function (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function which gives us the minibatches (both the datapoint and the corresponding label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get minibatches\n",
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9, X_val=None, y_val=None, Lambda=0, verb=True):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        # iterate over mini batches\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        # accuracy of model at end of epoch after all mini batch updates\n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        y_train_pred = []\n",
    "        y_val_pred = []\n",
    "        y_train1 = []\n",
    "        y_vall = []\n",
    "        for ii in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[ii:ii + minibatch_size, : ]\n",
    "            y_tr = y_train[ii:ii + minibatch_size,]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "        for ii in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[ii:ii + minibatch_size, : ]\n",
    "            y_va = y_val[ii:ii + minibatch_size,]\n",
    "            y_vall = np.append(y_vall, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
    "        \n",
    "        ## weights\n",
    "        w = np.array(net.params[0][0])\n",
    "        \n",
    "        ## adding regularization to cost\n",
    "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
    "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        if verb:\n",
    "            if i%50==0:\n",
    "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
    "    return net, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_accuracy(y_true, y_pred):\n",
    "    count = 0\n",
    "    for i,j in zip(y_true, y_pred):\n",
    "        if int(i)==j:\n",
    "            count +=1\n",
    "    return float(count)/float(len(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoking all that we have created until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "## input size\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "def train_and_test_loop(iterations, lr, Lambda, verb=True):\n",
    "    ## hyperparameters\n",
    "    iterations = iterations\n",
    "    learning_rate = lr\n",
    "    hidden_nodes = 10\n",
    "    output_nodes = 10\n",
    "\n",
    "    ## define neural net\n",
    "    nn = NN()\n",
    "    nn.add_layer(Linear(input_dim, hidden_nodes))\n",
    "\n",
    "    nn, val_acc = sgd(nn, X_train , y_train, minibatch_size=1000, epoch=iterations, learning_rate=learning_rate,\\\n",
    "                      X_val=X_val, y_val=y_val, Lambda=Lambda, verb=verb)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Check that the loss is reasonable : Disable the regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 0\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the loss range correct?? What about accuracy, does it make sense for an untrained network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets crank up the Lambda(Regularization)and check what it does to our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 4.56518470809e+101 | Training Accuracy = 0.0987166666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.098"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 1e3\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss went up. Good! (Another sanity check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets overfit to a small subset of our dataset, in this case 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_subset = X_train[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_subset = y_train[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train_subset\n",
    "y_train = y_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 784)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: Make sure that you can overfit very small portion of the training data\n",
    "So, set a small learning rate and turn regularization off\n",
    "\n",
    "In the code below:\n",
    "- Take the first 20 examples from MNIST\n",
    "- turn off regularization(reg=0.0)\n",
    "- use simple vanilla 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000: Loss = 2.29299230471 | Training Accuracy = 0.1\n",
      "Epoch 50/10000: Loss = 2.27716278532 | Training Accuracy = 0.2\n",
      "Epoch 100/10000: Loss = 2.26150682497 | Training Accuracy = 0.2\n",
      "Epoch 150/10000: Loss = 2.24602279271 | Training Accuracy = 0.25\n",
      "Epoch 200/10000: Loss = 2.23070900465 | Training Accuracy = 0.25\n",
      "Epoch 250/10000: Loss = 2.21556372339 | Training Accuracy = 0.4\n",
      "Epoch 300/10000: Loss = 2.20058515778 | Training Accuracy = 0.55\n",
      "Epoch 350/10000: Loss = 2.18577146342 | Training Accuracy = 0.5\n",
      "Epoch 400/10000: Loss = 2.17112074356 | Training Accuracy = 0.55\n",
      "Epoch 450/10000: Loss = 2.15663105054 | Training Accuracy = 0.55\n",
      "Epoch 500/10000: Loss = 2.14230038772 | Training Accuracy = 0.6\n",
      "Epoch 550/10000: Loss = 2.12812671186 | Training Accuracy = 0.65\n",
      "Epoch 600/10000: Loss = 2.11410793591 | Training Accuracy = 0.6\n",
      "Epoch 650/10000: Loss = 2.10024193214 | Training Accuracy = 0.65\n",
      "Epoch 700/10000: Loss = 2.08652653568 | Training Accuracy = 0.65\n",
      "Epoch 750/10000: Loss = 2.0729595483 | Training Accuracy = 0.65\n",
      "Epoch 800/10000: Loss = 2.05953874245 | Training Accuracy = 0.65\n",
      "Epoch 850/10000: Loss = 2.04626186553 | Training Accuracy = 0.65\n",
      "Epoch 900/10000: Loss = 2.03312664428 | Training Accuracy = 0.65\n",
      "Epoch 950/10000: Loss = 2.02013078931 | Training Accuracy = 0.7\n",
      "Epoch 1000/10000: Loss = 2.00727199969 | Training Accuracy = 0.7\n",
      "Epoch 1050/10000: Loss = 1.99454796755 | Training Accuracy = 0.7\n",
      "Epoch 1100/10000: Loss = 1.98195638266 | Training Accuracy = 0.7\n",
      "Epoch 1150/10000: Loss = 1.96949493699 | Training Accuracy = 0.7\n",
      "Epoch 1200/10000: Loss = 1.95716132908 | Training Accuracy = 0.7\n",
      "Epoch 1250/10000: Loss = 1.94495326841 | Training Accuracy = 0.7\n",
      "Epoch 1300/10000: Loss = 1.93286847946 | Training Accuracy = 0.7\n",
      "Epoch 1350/10000: Loss = 1.92090470569 | Training Accuracy = 0.7\n",
      "Epoch 1400/10000: Loss = 1.90905971327 | Training Accuracy = 0.7\n",
      "Epoch 1450/10000: Loss = 1.89733129457 | Training Accuracy = 0.7\n",
      "Epoch 1500/10000: Loss = 1.88571727139 | Training Accuracy = 0.7\n",
      "Epoch 1550/10000: Loss = 1.87421549797 | Training Accuracy = 0.7\n",
      "Epoch 1600/10000: Loss = 1.86282386368 | Training Accuracy = 0.7\n",
      "Epoch 1650/10000: Loss = 1.8515402955 | Training Accuracy = 0.7\n",
      "Epoch 1700/10000: Loss = 1.84036276015 | Training Accuracy = 0.7\n",
      "Epoch 1750/10000: Loss = 1.82928926601 | Training Accuracy = 0.7\n",
      "Epoch 1800/10000: Loss = 1.81831786471 | Training Accuracy = 0.7\n",
      "Epoch 1850/10000: Loss = 1.80744665253 | Training Accuracy = 0.7\n",
      "Epoch 1900/10000: Loss = 1.79667377148 | Training Accuracy = 0.7\n",
      "Epoch 1950/10000: Loss = 1.78599741016 | Training Accuracy = 0.7\n",
      "Epoch 2000/10000: Loss = 1.77541580436 | Training Accuracy = 0.7\n",
      "Epoch 2050/10000: Loss = 1.76492723752 | Training Accuracy = 0.7\n",
      "Epoch 2100/10000: Loss = 1.75453004087 | Training Accuracy = 0.7\n",
      "Epoch 2150/10000: Loss = 1.74422259345 | Training Accuracy = 0.7\n",
      "Epoch 2200/10000: Loss = 1.73400332195 | Training Accuracy = 0.7\n",
      "Epoch 2250/10000: Loss = 1.72387070035 | Training Accuracy = 0.7\n",
      "Epoch 2300/10000: Loss = 1.71382324944 | Training Accuracy = 0.7\n",
      "Epoch 2350/10000: Loss = 1.70385953621 | Training Accuracy = 0.7\n",
      "Epoch 2400/10000: Loss = 1.6939781731 | Training Accuracy = 0.7\n",
      "Epoch 2450/10000: Loss = 1.68417781714 | Training Accuracy = 0.7\n",
      "Epoch 2500/10000: Loss = 1.67445716906 | Training Accuracy = 0.7\n",
      "Epoch 2550/10000: Loss = 1.66481497225 | Training Accuracy = 0.7\n",
      "Epoch 2600/10000: Loss = 1.65525001168 | Training Accuracy = 0.7\n",
      "Epoch 2650/10000: Loss = 1.64576111278 | Training Accuracy = 0.7\n",
      "Epoch 2700/10000: Loss = 1.63634714025 | Training Accuracy = 0.75\n",
      "Epoch 2750/10000: Loss = 1.62700699688 | Training Accuracy = 0.75\n",
      "Epoch 2800/10000: Loss = 1.61773962227 | Training Accuracy = 0.75\n",
      "Epoch 2850/10000: Loss = 1.60854399165 | Training Accuracy = 0.75\n",
      "Epoch 2900/10000: Loss = 1.59941911455 | Training Accuracy = 0.75\n",
      "Epoch 2950/10000: Loss = 1.5903640336 | Training Accuracy = 0.75\n",
      "Epoch 3000/10000: Loss = 1.58137782324 | Training Accuracy = 0.75\n",
      "Epoch 3050/10000: Loss = 1.57245958851 | Training Accuracy = 0.75\n",
      "Epoch 3100/10000: Loss = 1.56360846378 | Training Accuracy = 0.75\n",
      "Epoch 3150/10000: Loss = 1.5548236116 | Training Accuracy = 0.75\n",
      "Epoch 3200/10000: Loss = 1.54610422146 | Training Accuracy = 0.75\n",
      "Epoch 3250/10000: Loss = 1.53744950867 | Training Accuracy = 0.75\n",
      "Epoch 3300/10000: Loss = 1.52885871321 | Training Accuracy = 0.75\n",
      "Epoch 3350/10000: Loss = 1.52033109864 | Training Accuracy = 0.75\n",
      "Epoch 3400/10000: Loss = 1.51186595102 | Training Accuracy = 0.75\n",
      "Epoch 3450/10000: Loss = 1.50346257792 | Training Accuracy = 0.75\n",
      "Epoch 3500/10000: Loss = 1.49512030736 | Training Accuracy = 0.75\n",
      "Epoch 3550/10000: Loss = 1.4868384869 | Training Accuracy = 0.75\n",
      "Epoch 3600/10000: Loss = 1.47861648267 | Training Accuracy = 0.75\n",
      "Epoch 3650/10000: Loss = 1.47045367852 | Training Accuracy = 0.75\n",
      "Epoch 3700/10000: Loss = 1.46234947516 | Training Accuracy = 0.75\n",
      "Epoch 3750/10000: Loss = 1.45430328931 | Training Accuracy = 0.75\n",
      "Epoch 3800/10000: Loss = 1.44631455293 | Training Accuracy = 0.85\n",
      "Epoch 3850/10000: Loss = 1.4383827125 | Training Accuracy = 0.85\n",
      "Epoch 3900/10000: Loss = 1.43050722826 | Training Accuracy = 0.85\n",
      "Epoch 3950/10000: Loss = 1.42268757358 | Training Accuracy = 0.85\n",
      "Epoch 4000/10000: Loss = 1.41492323425 | Training Accuracy = 0.85\n",
      "Epoch 4050/10000: Loss = 1.40721370792 | Training Accuracy = 0.85\n",
      "Epoch 4100/10000: Loss = 1.39955850346 | Training Accuracy = 0.85\n",
      "Epoch 4150/10000: Loss = 1.39195714045 | Training Accuracy = 0.85\n",
      "Epoch 4200/10000: Loss = 1.38440914863 | Training Accuracy = 0.85\n",
      "Epoch 4250/10000: Loss = 1.37691406737 | Training Accuracy = 0.85\n",
      "Epoch 4300/10000: Loss = 1.36947144527 | Training Accuracy = 0.85\n",
      "Epoch 4350/10000: Loss = 1.3620808396 | Training Accuracy = 0.85\n",
      "Epoch 4400/10000: Loss = 1.354741816 | Training Accuracy = 0.85\n",
      "Epoch 4450/10000: Loss = 1.34745394795 | Training Accuracy = 0.9\n",
      "Epoch 4500/10000: Loss = 1.34021681649 | Training Accuracy = 0.9\n",
      "Epoch 4550/10000: Loss = 1.3330300098 | Training Accuracy = 0.9\n",
      "Epoch 4600/10000: Loss = 1.3258931229 | Training Accuracy = 0.9\n",
      "Epoch 4650/10000: Loss = 1.3188057573 | Training Accuracy = 0.9\n",
      "Epoch 4700/10000: Loss = 1.31176752069 | Training Accuracy = 0.9\n",
      "Epoch 4750/10000: Loss = 1.3047780267 | Training Accuracy = 0.9\n",
      "Epoch 4800/10000: Loss = 1.29783689457 | Training Accuracy = 0.9\n",
      "Epoch 4850/10000: Loss = 1.29094374896 | Training Accuracy = 0.9\n",
      "Epoch 4900/10000: Loss = 1.28409821967 | Training Accuracy = 0.95\n",
      "Epoch 4950/10000: Loss = 1.27729994141 | Training Accuracy = 0.95\n",
      "Epoch 5000/10000: Loss = 1.27054855363 | Training Accuracy = 0.95\n",
      "Epoch 5050/10000: Loss = 1.26384370027 | Training Accuracy = 0.95\n",
      "Epoch 5100/10000: Loss = 1.25718502961 | Training Accuracy = 0.95\n",
      "Epoch 5150/10000: Loss = 1.25057219409 | Training Accuracy = 0.95\n",
      "Epoch 5200/10000: Loss = 1.24400485011 | Training Accuracy = 0.95\n",
      "Epoch 5250/10000: Loss = 1.23748265793 | Training Accuracy = 0.95\n",
      "Epoch 5300/10000: Loss = 1.23100528148 | Training Accuracy = 0.95\n",
      "Epoch 5350/10000: Loss = 1.22457238824 | Training Accuracy = 0.95\n",
      "Epoch 5400/10000: Loss = 1.21818364909 | Training Accuracy = 0.95\n",
      "Epoch 5450/10000: Loss = 1.21183873825 | Training Accuracy = 0.95\n",
      "Epoch 5500/10000: Loss = 1.20553733308 | Training Accuracy = 0.95\n",
      "Epoch 5550/10000: Loss = 1.19927911403 | Training Accuracy = 0.95\n",
      "Epoch 5600/10000: Loss = 1.19306376454 | Training Accuracy = 0.95\n",
      "Epoch 5650/10000: Loss = 1.18689097091 | Training Accuracy = 0.95\n",
      "Epoch 5700/10000: Loss = 1.18076042224 | Training Accuracy = 0.95\n",
      "Epoch 5750/10000: Loss = 1.17467181036 | Training Accuracy = 0.95\n",
      "Epoch 5800/10000: Loss = 1.16862482971 | Training Accuracy = 0.95\n",
      "Epoch 5850/10000: Loss = 1.16261917729 | Training Accuracy = 0.95\n",
      "Epoch 5900/10000: Loss = 1.15665455261 | Training Accuracy = 0.95\n",
      "Epoch 5950/10000: Loss = 1.15073065758 | Training Accuracy = 0.95\n",
      "Epoch 6000/10000: Loss = 1.1448471965 | Training Accuracy = 0.95\n",
      "Epoch 6050/10000: Loss = 1.13900387598 | Training Accuracy = 0.95\n",
      "Epoch 6100/10000: Loss = 1.13320040487 | Training Accuracy = 0.95\n",
      "Epoch 6150/10000: Loss = 1.12743649424 | Training Accuracy = 0.95\n",
      "Epoch 6200/10000: Loss = 1.12171185732 | Training Accuracy = 0.95\n",
      "Epoch 6250/10000: Loss = 1.11602620946 | Training Accuracy = 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6300/10000: Loss = 1.11037926809 | Training Accuracy = 0.95\n",
      "Epoch 6350/10000: Loss = 1.10477075265 | Training Accuracy = 0.95\n",
      "Epoch 6400/10000: Loss = 1.09920038463 | Training Accuracy = 0.95\n",
      "Epoch 6450/10000: Loss = 1.09366788744 | Training Accuracy = 0.95\n",
      "Epoch 6500/10000: Loss = 1.08817298647 | Training Accuracy = 0.95\n",
      "Epoch 6550/10000: Loss = 1.08271540898 | Training Accuracy = 0.95\n",
      "Epoch 6600/10000: Loss = 1.07729488413 | Training Accuracy = 0.95\n",
      "Epoch 6650/10000: Loss = 1.07191114294 | Training Accuracy = 0.95\n",
      "Epoch 6700/10000: Loss = 1.06656391822 | Training Accuracy = 0.95\n",
      "Epoch 6750/10000: Loss = 1.06125294464 | Training Accuracy = 0.95\n",
      "Epoch 6800/10000: Loss = 1.05597795861 | Training Accuracy = 0.95\n",
      "Epoch 6850/10000: Loss = 1.05073869831 | Training Accuracy = 0.95\n",
      "Epoch 6900/10000: Loss = 1.04553490369 | Training Accuracy = 0.95\n",
      "Epoch 6950/10000: Loss = 1.04036631639 | Training Accuracy = 0.95\n",
      "Epoch 7000/10000: Loss = 1.03523267978 | Training Accuracy = 0.95\n",
      "Epoch 7050/10000: Loss = 1.03013373893 | Training Accuracy = 1.0\n",
      "Epoch 7100/10000: Loss = 1.02506924056 | Training Accuracy = 1.0\n",
      "Epoch 7150/10000: Loss = 1.02003893307 | Training Accuracy = 1.0\n",
      "Epoch 7200/10000: Loss = 1.01504256652 | Training Accuracy = 1.0\n",
      "Epoch 7250/10000: Loss = 1.01007989259 | Training Accuracy = 1.0\n",
      "Epoch 7300/10000: Loss = 1.00515066459 | Training Accuracy = 1.0\n",
      "Epoch 7350/10000: Loss = 1.00025463744 | Training Accuracy = 1.0\n",
      "Epoch 7400/10000: Loss = 0.995391567679 | Training Accuracy = 1.0\n",
      "Epoch 7450/10000: Loss = 0.990561213416 | Training Accuracy = 1.0\n",
      "Epoch 7500/10000: Loss = 0.985763334357 | Training Accuracy = 1.0\n",
      "Epoch 7550/10000: Loss = 0.980997691778 | Training Accuracy = 1.0\n",
      "Epoch 7600/10000: Loss = 0.97626404852 | Training Accuracy = 1.0\n",
      "Epoch 7650/10000: Loss = 0.971562168979 | Training Accuracy = 1.0\n",
      "Epoch 7700/10000: Loss = 0.966891819099 | Training Accuracy = 1.0\n",
      "Epoch 7750/10000: Loss = 0.962252766366 | Training Accuracy = 1.0\n",
      "Epoch 7800/10000: Loss = 0.957644779795 | Training Accuracy = 1.0\n",
      "Epoch 7850/10000: Loss = 0.953067629929 | Training Accuracy = 1.0\n",
      "Epoch 7900/10000: Loss = 0.948521088828 | Training Accuracy = 1.0\n",
      "Epoch 7950/10000: Loss = 0.944004930064 | Training Accuracy = 1.0\n",
      "Epoch 8000/10000: Loss = 0.939518928713 | Training Accuracy = 1.0\n",
      "Epoch 8050/10000: Loss = 0.935062861351 | Training Accuracy = 1.0\n",
      "Epoch 8100/10000: Loss = 0.930636506044 | Training Accuracy = 1.0\n",
      "Epoch 8150/10000: Loss = 0.926239642347 | Training Accuracy = 1.0\n",
      "Epoch 8200/10000: Loss = 0.921872051292 | Training Accuracy = 1.0\n",
      "Epoch 8250/10000: Loss = 0.917533515387 | Training Accuracy = 1.0\n",
      "Epoch 8300/10000: Loss = 0.913223818609 | Training Accuracy = 1.0\n",
      "Epoch 8350/10000: Loss = 0.908942746396 | Training Accuracy = 1.0\n",
      "Epoch 8400/10000: Loss = 0.904690085644 | Training Accuracy = 1.0\n",
      "Epoch 8450/10000: Loss = 0.900465624701 | Training Accuracy = 1.0\n",
      "Epoch 8500/10000: Loss = 0.896269153362 | Training Accuracy = 1.0\n",
      "Epoch 8550/10000: Loss = 0.89210046286 | Training Accuracy = 1.0\n",
      "Epoch 8600/10000: Loss = 0.887959345866 | Training Accuracy = 1.0\n",
      "Epoch 8650/10000: Loss = 0.883845596479 | Training Accuracy = 1.0\n",
      "Epoch 8700/10000: Loss = 0.879759010224 | Training Accuracy = 1.0\n",
      "Epoch 8750/10000: Loss = 0.875699384042 | Training Accuracy = 1.0\n",
      "Epoch 8800/10000: Loss = 0.871666516292 | Training Accuracy = 1.0\n",
      "Epoch 8850/10000: Loss = 0.867660206737 | Training Accuracy = 1.0\n",
      "Epoch 8900/10000: Loss = 0.863680256546 | Training Accuracy = 1.0\n",
      "Epoch 8950/10000: Loss = 0.859726468283 | Training Accuracy = 1.0\n",
      "Epoch 9000/10000: Loss = 0.855798645906 | Training Accuracy = 1.0\n",
      "Epoch 9050/10000: Loss = 0.851896594757 | Training Accuracy = 1.0\n",
      "Epoch 9100/10000: Loss = 0.848020121561 | Training Accuracy = 1.0\n",
      "Epoch 9150/10000: Loss = 0.844169034418 | Training Accuracy = 1.0\n",
      "Epoch 9200/10000: Loss = 0.840343142796 | Training Accuracy = 1.0\n",
      "Epoch 9250/10000: Loss = 0.83654225753 | Training Accuracy = 1.0\n",
      "Epoch 9300/10000: Loss = 0.83276619081 | Training Accuracy = 1.0\n",
      "Epoch 9350/10000: Loss = 0.829014756183 | Training Accuracy = 1.0\n",
      "Epoch 9400/10000: Loss = 0.825287768538 | Training Accuracy = 1.0\n",
      "Epoch 9450/10000: Loss = 0.82158504411 | Training Accuracy = 1.0\n",
      "Epoch 9500/10000: Loss = 0.817906400467 | Training Accuracy = 1.0\n",
      "Epoch 9550/10000: Loss = 0.814251656506 | Training Accuracy = 1.0\n",
      "Epoch 9600/10000: Loss = 0.810620632449 | Training Accuracy = 1.0\n",
      "Epoch 9650/10000: Loss = 0.807013149835 | Training Accuracy = 1.0\n",
      "Epoch 9700/10000: Loss = 0.803429031513 | Training Accuracy = 1.0\n",
      "Epoch 9750/10000: Loss = 0.799868101641 | Training Accuracy = 1.0\n",
      "Epoch 9800/10000: Loss = 0.796330185672 | Training Accuracy = 1.0\n",
      "Epoch 9850/10000: Loss = 0.792815110355 | Training Accuracy = 1.0\n",
      "Epoch 9900/10000: Loss = 0.789322703726 | Training Accuracy = 1.0\n",
      "Epoch 9950/10000: Loss = 0.7858527951 | Training Accuracy = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4718"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "Lambda = 0\n",
    "train_and_test_loop(10000, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very small loss,  train accuracy 100, nice! We are successful in overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the original dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "(train_features, train_targets), (test_features, test_targets) = mnist.load_data()\n",
    "\n",
    "train_features = train_features.reshape(60000, 784)\n",
    "print train_features.shape\n",
    "test_features = test_features.reshape(10000, 784)\n",
    "print test_features.shape\n",
    "\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "train_features = train_features / 255.0\n",
    "test_features = test_features / 255.0\n",
    "\n",
    "print train_targets.shape\n",
    "print test_targets.shape\n",
    "\n",
    "X_train = train_features\n",
    "y_train = train_targets\n",
    "\n",
    "X_val = test_features\n",
    "y_val = test_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with small regularization and find learning rate that makes the loss go down.\n",
    "\n",
    "- we start with Lambda(small regularization) = 1e-7\n",
    "- we start with a small learning rate = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 2.3009780135 | Training Accuracy = 0.133566666667\n",
      "Epoch 50/500: Loss = 2.30064390573 | Training Accuracy = 0.13435\n",
      "Epoch 100/500: Loss = 2.30030992169 | Training Accuracy = 0.134933333333\n",
      "Epoch 150/500: Loss = 2.29997606121 | Training Accuracy = 0.135783333333\n",
      "Epoch 200/500: Loss = 2.29964232411 | Training Accuracy = 0.1367\n",
      "Epoch 250/500: Loss = 2.29930871022 | Training Accuracy = 0.137316666667\n",
      "Epoch 300/500: Loss = 2.29897521936 | Training Accuracy = 0.138033333333\n",
      "Epoch 350/500: Loss = 2.29864185136 | Training Accuracy = 0.138766666667\n",
      "Epoch 400/500: Loss = 2.29830860604 | Training Accuracy = 0.139466666667\n",
      "Epoch 450/500: Loss = 2.29797548323 | Training Accuracy = 0.140166666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.138"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss barely changing. Learning rate is probably too low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay now lets try a (larger) learning rate 1e6. What could possibly go wrong?\n",
    "\n",
    "- Learning rate lr = 1e6\n",
    "- Regularization lambda = 1e-7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 4.57097312616e+220 | Training Accuracy = 0.0987166666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/numpy/core/_methods.py:32: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n",
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in square\n",
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/numpy/core/_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 100/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 150/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 200/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 250/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 300/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 350/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 400/500: Loss = nan | Training Accuracy = 0.0987166666667\n",
      "Epoch 450/500: Loss = nan | Training Accuracy = 0.0987166666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.098"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e6\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss exploding. Learning rate is too high. \n",
    "Cost is very high. Always means high learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to train now with a value of learning rate between 1e-7 and 1e6\n",
    "\n",
    "- learning rate = 1e4\n",
    "- regularization remains the small, lambda = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 8.85131574014 | Training Accuracy = 0.899633333333\n",
      "Epoch 50/500: Loss = 6.84291364561 | Training Accuracy = 0.898283333333\n",
      "Epoch 100/500: Loss = 8.08382855856 | Training Accuracy = 0.898416666667\n",
      "Epoch 150/500: Loss = 9.63714543264 | Training Accuracy = 0.892716666667\n",
      "Epoch 200/500: Loss = 10.7163696015 | Training Accuracy = 0.9062\n",
      "Epoch 250/500: Loss = 11.7163368671 | Training Accuracy = 0.901616666667\n",
      "Epoch 300/500: Loss = 12.914686422 | Training Accuracy = 0.898016666667\n",
      "Epoch 350/500: Loss = 13.9447751067 | Training Accuracy = 0.88805\n",
      "Epoch 400/500: Loss = 15.3446817515 | Training Accuracy = 0.904366666667\n",
      "Epoch 450/500: Loss = 16.115403978 | Training Accuracy = 0.897116666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8754"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e4\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still too high learning rate. Loss is not decreasing. The rough range of learning rate we should be cross validating is somewhere between [1e4 to 1e-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "### Cross validation Strategy\n",
    "\n",
    "\n",
    "- Do coarse -> fine cross-validation in stages\n",
    "\n",
    "- First stage: only a few epochs to get rough idea of what params work\n",
    "- Second stage: longer running time, finer search\n",
    "- â€¦ (repeat as necessary)\n",
    "\n",
    "### Tip for detecting explosions in the solver: \n",
    "- If the cost is ever > 3 * original cost, break out early\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For example: Run coarse search for 100 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/100: Best_val_acc: 0.8398, lr: 7.45468488425e-05, Lambda: 0.000159903616103\n",
      "\n",
      "Try 2/100: Best_val_acc: 0.5706, lr: 3.56991738793e-06, Lambda: 20.1237892817\n",
      "\n",
      "Try 3/100: Best_val_acc: 0.9223, lr: 0.00992739351399, Lambda: 0.0021095154455\n",
      "\n",
      "Try 4/100: Best_val_acc: 0.1022, lr: 1.78885786607e-07, Lambda: 0.00309312931524\n",
      "\n",
      "Try 5/100: Best_val_acc: 0.8975, lr: 0.00076891785533, Lambda: 0.448688796025\n",
      "\n",
      "Try 6/100: Best_val_acc: 0.9063, lr: 0.00133006129195, Lambda: 2.70937081705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 7/100: Best_val_acc: 0.098, lr: 4.32965252147, Lambda: 18924.5154169\n",
      "\n",
      "Try 8/100: Best_val_acc: 0.9026, lr: 0.00107923301317, Lambda: 2.13502220135\n",
      "\n",
      "Try 9/100: Best_val_acc: 0.098, lr: 0.00192349535701, Lambda: 4637.1944929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,100):\n",
    "    lr = math.pow(10, np.random.uniform(-7.0, 4.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,5))\n",
    "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run finer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/100: Best_val_acc: 0.9218, lr: 0.0115688318738, Lambda: 0.105897187144\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: overflow encountered in square\n",
      "/usr/local/anaconda/python2/lib/python2.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in subtract\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 2/100: Best_val_acc: 0.098, lr: 0.0531240091238, Lambda: 91.7611762264\n",
      "\n",
      "Try 3/100: Best_val_acc: 0.8841, lr: 0.000370529160594, Lambda: 0.0311068816491\n",
      "\n",
      "Try 4/100: Best_val_acc: 0.769, lr: 1.82585492168e-05, Lambda: 0.685726311211\n",
      "\n",
      "Try 5/100: Best_val_acc: 0.9232, lr: 0.0266641552965, Lambda: 0.00610437364682\n",
      "\n",
      "Try 6/100: Best_val_acc: 0.9163, lr: 0.00314008487927, Lambda: 0.841207757356\n",
      "\n",
      "Try 7/100: Best_val_acc: 0.098, lr: 0.00594118250727, Lambda: 51.894609946\n",
      "\n",
      "Try 8/100: Best_val_acc: 0.9182, lr: 0.0052358650493, Lambda: 0.000602817654684\n",
      "\n",
      "Try 9/100: Best_val_acc: 0.9232, lr: 0.0183392121727, Lambda: 0.0191855165621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,2))\n",
    "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
