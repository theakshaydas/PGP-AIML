{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_linear(x,w): # Model\n",
    "    f = w[1]*x + w[0] # Linear predictor\n",
    "    return f\n",
    "\n",
    "def loss_MSE(f,y):\n",
    "    loss = np.sum(np.square(y-f)) / np.size(f)\n",
    "    return loss\n",
    "\n",
    "def loss_MAE(f,y):\n",
    "    loss = np.sum(np.abs(y-f)) / np.size(f)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'f(x) and y')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COOK UP DATA\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) # Inputs\n",
    "w_ideal = np.array([3, -2]) # Actual weights\n",
    "y = w_ideal[1]*x + w_ideal[0] + 0.9*np.random.randn(np.size(x)) # Adding some noise to desired output\n",
    "\n",
    "# TRY THE IDEAL WEIGHTS AND PLOT DATA\n",
    "w = w_ideal # Some estimated weights.\n",
    "f = model_linear(x,w) # Model\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(x, y, c=\"b\", marker='x')\n",
    "plt.plot(x, f, c=\"r\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x) and y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15957771183\n"
     ]
    }
   ],
   "source": [
    "# CHECK THE LOSS WITH IDEAL WEIGHTS. IT SHOULD BE LOW, IF THE NOISE IS LOW.\n",
    "\n",
    "loss = loss_MSE(f,y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW DEVIATE FROM THE IDEAL WEIGHTS. LOSS SHOULD INCREASE.\n",
    "\n",
    "w = np.array([3.5, -1.9]) # Some estimated weights.\n",
    "f = model_linear(x,w) # Model \n",
    "loss = loss_MSE(f,y)\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY VERY DIFFERENT WEIGHTS. LOSS SHOULD BE LARGE.\n",
    "w = np.array([32, 2.1]) # Some estimated weights.\n",
    "f = model_linear(x,w) # Model \n",
    "loss = loss_MSE(f,y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD AN OUTLIER AND CHECK MSE LOSS WITH IDEAL WEIGHTS. \n",
    "x = np.append(x,[10])\n",
    "y = np.append(y,[108])\n",
    "\n",
    "w = w_ideal # Some estimated weights.\n",
    "f = model_linear(x,w) # Model\n",
    "\n",
    "loss = loss_MSE(f,y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY MAE LOSS, WHICH SHOULD BE MUCH SMALLER\n",
    "loss = loss_MAE(f,y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Gradient Descent on MSE loss\n",
    "\n",
    "# DEFINE GRADIENT\n",
    "\n",
    "def grad2(w, x, y):\n",
    "    x1 = np.vstack((x, np.ones_like(x)))\n",
    "    f = w.dot(x1).flatten()\n",
    "    error = (y.flatten() - f)\n",
    "    gradient = -(1.0/len(x)) * x1.dot(error)\n",
    "    return gradient, f\n",
    "\n",
    "\n",
    "# GRADIENT DESCENT\n",
    "\n",
    "w = np.array([10, 2.1]) # Some estimated weights.\n",
    "learning_rate = 0.0005\n",
    "min_abs_change = 1e-5\n",
    "max_iter = 10000\n",
    "\n",
    "iterations = 1\n",
    "while True:\n",
    "    gradient, f = grad2(w, x, y)\n",
    "    w_new = w - learning_rate * gradient\n",
    "    \n",
    "    # Stopping Condition\n",
    "    if np.sum(abs(w_new - w)) < min_abs_change:\n",
    "        break\n",
    "    if iterations > max_iter:\n",
    "        break\n",
    "    \n",
    "    if iterations % (max_iter/10) == 0:\n",
    "        # Plot data\n",
    "        plt.scatter(x, y, c=\"b\", marker='x')\n",
    "        plt.plot(x, f, c=\"r\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"f(x) and y\")\n",
    "    \n",
    "    iterations += 1\n",
    "    w = w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent on MAE loss\n",
    "\n",
    "def grad1(w, x, y):\n",
    "    x1 = np.vstack((x, np.ones_like(x)))\n",
    "    f = w.dot(x1).flatten()    \n",
    "    error = (y.flatten() - f)\n",
    "    gradient = -(1.0/len(x)) * x.dot(np.sign(error))\n",
    "    return gradient, f\n",
    "\n",
    "\n",
    "# GRADIENT DESCENT\n",
    "\n",
    "w = np.array([10, 2.1]) # Some estimated weights.\n",
    "learning_rate = 0.0005\n",
    "min_abs_change = 1e-5\n",
    "max_iter = 10000\n",
    "\n",
    "iterations = 1\n",
    "while True:\n",
    "    gradient, f = grad1(w, x, y)\n",
    "    w_new = w - learning_rate * gradient\n",
    "    \n",
    "    # Stopping Condition\n",
    "    if np.sum(abs(w_new - w)) < min_abs_change:\n",
    "        break\n",
    "    if iterations > max_iter:\n",
    "        break\n",
    "    \n",
    "    if iterations % (max_iter/10) == 0:\n",
    "        # Plot data\n",
    "        plt.scatter(x, y, c=\"b\", marker='x')\n",
    "        plt.plot(x, f, c=\"r\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"f(x) and y\")\n",
    "    \n",
    "    iterations += 1\n",
    "    w = w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform L2 regularized L2 loss (MSE) gradient descent\n",
    "\n",
    "def grad2L2(w, x, y, l):\n",
    "    x1 = np.vstack((x, np.ones_like(x)))\n",
    "    f = w.dot(x1).flatten()\n",
    "    error = (y.flatten() - f)\n",
    "    gradient = -(1.0/len(x)) * x1.dot(error) +  l * np.hstack((w[0,],np.zeros_like(w[1])))\n",
    "    return gradient, f\n",
    "\n",
    "\n",
    "# GRADIENT DESCENT\n",
    "\n",
    "w = np.array([10, 2.1]) # Some estimated weights.\n",
    "learning_rate = 0.0005\n",
    "min_abs_change = 1e-5\n",
    "max_iter = 10000\n",
    "l = 1000\n",
    "\n",
    "iterations = 1\n",
    "while True:\n",
    "    gradient, f = grad2L2(w, x, y, l)\n",
    "    w_new = w - learning_rate * gradient\n",
    "    \n",
    "    # Stopping Condition\n",
    "    if np.sum(abs(w_new - w)) < min_abs_change:\n",
    "        break\n",
    "    if iterations > max_iter:\n",
    "        break\n",
    "    \n",
    "    if iterations % (max_iter/10) == 0:\n",
    "        # Plot data\n",
    "        plt.scatter(x, y, c=\"b\", marker='x')\n",
    "        plt.plot(x, f, c=\"r\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"f(x) and y\")\n",
    "    \n",
    "    iterations += 1\n",
    "    w = w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
